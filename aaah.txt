def model_select(args):
    model_name = args.model
    dataset_name = args.dataset

    if dataset_name == 'mnist':
        unsupervised_dataset, supervised_dataset, validation_dataset, test_dataset = \
            load_MNIST_data(args.num_supervised, args.num_unsupervised, True, True)
    else:
        unsupervised_dataset = None if args.unsupervised_file is None else load_data_from_file(args.unsupervised_file)
        supervised_dataset = load_data_from_file(args.supervised_file)

    supervised_dataloader = DataLoader(supervised_dataset, batch_size=args.supervised_batch_size, shuffle=True)

    if model_name == 'simple':
        model = SimpleNetwork(784, [1000, 500, 250, 250, 250], 10, dataset_name, device)
        dataloaders = (supervised_dataloader,)

    elif model_name == 'pretraining':
        model = PretrainingNetwork(784, [1000, 500, 250, 250, 250], 10, lambda x: x, nn.Sigmoid(), dataset_name, device)

    elif model_name == 'sdae':
        model = SDAE(784, [1000, 500, 250, 250, 250], 10, nn.ReLU(), dataset_name, device)

    elif model_name == 'simple_m1':
        model = SimpleM1(784, [256, 128], 32, [32], 10, lambda x: x, nn.Sigmoid(), dataset_name, device)

    elif model_name == 'm1':
        model = M1(784, [256, 128], 32, [32], 10, nn.Sigmoid(), dataset_name, device)

    elif model_name == 'm2':
        model = M2Runner(784, [256, 128], [256], 32, 10, nn.Sigmoid(), dataset_name, device)
        unsupervised_dataloader = DataLoader(unsupervised_dataset, batch_size=batch_size, shuffle=True)
        dataloaders = (supervised_dataloader, un, validation_dataloader)

    elif model_name == 'ladder':
        model = LadderNetwork(784, [1000, 500, 250, 250, 250], 10, [1000.0, 10.0, 0.10, 0.10, 0.10, 0.10, 0.10],
                              dataset_name, device)
        un = DataLoader(unsupervised_dataset, batch_size=batch_size, shuffle=True)
        dataloaders = (supervised_dataloader, un, validation_dataloader)

# parser.add_argument("--unsupervised_file", type=str,
    #                         help="Relative path to file containing data for unsupervised training")
    # parser.add_argument("supervised_data_file", type=str,
    #                         help="Relative path to file containing the input data for supervised training")
    # parser.add_argument("supervised_labels_file", type=str,
    #                         help="Relative path to file containing the output data for supervised training")
    # parser.add_argument("--model", dest="unsupervised_model_file", type=str, default="model.pt",
    #                     help="Relative path to file to store the trained unsupervised model")
    # parser.add_argument("--batch_size", dest="batch_size", type=int, default=500, help="Batch size to use in training")
    # parser.add_argument("--epochs_unsupervised", dest="num_epochs_unsupervised", type=int, default=100,
    #                     help="Number of epochs to train for unsupervised")
    # parser.add_argument("--epochs_supervised", dest="num_epochs_supervised", type=int, default=100,
    #                     help="Number of epochs to train for supervised")
    # parser.add_argument("--num_folds", dest="num_folds", type=int, default=10,
    #                     help="Number of folds for cross validation")
    # parser.add_argument("--learning_rate", dest="learning_rate", type=float, default=1e-3,
    #                     help="Learning rate for gradient descent")